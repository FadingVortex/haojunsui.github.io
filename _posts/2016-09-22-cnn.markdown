---
layout:     post
title:      "Classifying Images using a Convolutional Neural Network"
subtitle:   "Convolutional Neural Network"
date:       2016-09-26 05:00:00
author:     "Haojun"
header-img: "img/in-post/cnn/header.jpg"
catalog:    true
tags:
    - Computer Vision
    - Convolutional Neural Network
    - Matlab
---

<script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

## Introduction

#### Task Performed

In this project we created a convolutional neural network and used it to classify the CIFAR-10 dataset. We created a convolutional neural network using eighteen layers, consisting of six layer types. The layer types were image normalization, convolution, rectified linear units, maxpool, fully connected, and softmax. In the following report, we describe each of these different layers types in detail. We also describe and show observations and results for the CNN, classifying the CIFAR-10 images using provided filters and bias values. Finally, we test our CNN on external images, found online, and present our findings. We hope to achieve a better understanding of convolutional neural networks and their implementation after completing this report. For this specific project, we wish to see that our network will have a good prediction rate accuracy.

## Layer Types & Description

#### Image Normalization

The first step in our convolutional neural network is to normalize the input image we are given. Images are normally given as a set of RGB values. These values range from 0 to 255 for each RGB channel. We then want to normalize all of these values to a range from -0.5 to 0.5. To do this we simply can take the input image and apply the following transformation on it:

$$Out(i,j,k)=\frac{In(i,j,k)}{255.0}-0.5$$

For example, let’s say we were given an image pixel with the following values, (R, G, B) = (10, 128, 20). Let us apply this image normalization technique to see what we get. So we will have (10, 128, 20)/255.0 − 0.5, which gives us (−0.4608,0.0020,−0.4216) as our normalized values. As we can see all the values are are between -0.5 and 0.5. Also, notice that the G value was 128 which is nearly half of 255 and as a result we get a value very close to zero after normalization. We can now used this normalized value in the rest of our convolutional neural network. Normalization of data is a necessary and useful technique in most machine learning applications. In MATLAB, this step can be easily implemented and follows exactly from the transformation we have just described. We will just take in the input array and divide by 255 and subtract by 0.5.

#### Rectified Linear Unit
We use rectified linear units (ReLUs) to threshold the input. These are usually the next layer after convolutions in convolutional neural networks.  We define ReLUs the following way:

$$Out(i,j,k)=max(In(i,j,k),0)$$

So for any input we are given, we will make sure the output always has values 0 or greater. We will demonstrate this technique by showing you an example on a matrix. We have our original matrix, A, defined as follows:

$$
A=
\begin{bmatrix}
	0.4 & 0.3 & -0.1\\
	0 & 0.1 & -0.2\\
	-0.3 & -0.4 & 0.2\\
\end{bmatrix}
$$

After we apply ReLU(A), we have the result below. The highlighted cells show where the actual thresholding has occurred. All of the negative values in our original matrix have now been set to 0 and everything else has been left alone.

$$
ReLU(A)=
\begin{bmatrix}
	0.4 & 0.3 & 0\\
	0 & 0.1 & 0\\
	0 & 0 & 0.2\\
\end{bmatrix}
$$

In MATLAB, this can be implemented in one line using the max function of the input array and 0. You can see from this that MATLAB allows us to easily implement many of these functions in the way they are mathematically defined.

There are many other types of activation functions such as sigmoid, tanh, and maxout. ReLUs have been adopted recently, since they are easy to compute and they lead to faster convergence. In other activation functions, we are forced to compute exponential values which are a lot more computation- ally heavy than performing a max function. One problem with ReLUs has been the fact that neurons can stop activating with ReLUs. Once a neuron is not activating it is basically a useless neuron in the network. Leaky ReLUs have been used to combat this and are defined as follows:

$$Out(i,j,k)=max(In(i,j,k),In(i,j,k) \times 0.01)$$

#### Maxpool

The maxpool operation is used to down-sample the images. This takes inaninputofsize2N×2M×DandturnsitintoofsizeN×M×D. So it will take in an input of and reduce the number of rows and columns by 2 but we will not change the depth of the input. The maxpool operation is beneficial because it allows us to not overfit to our data and it also allows us to compute a lot less. This is very beneficial to use because training deep networks takes a lot of time and if we have less parameters it will be faster to convergence. We define the maxpool operation as follows:

$$Out(i,j,k)=max(\left\{In(r,c,k) \mid (2i-1)<r<2i\mbox{~and~}(2j-1)<c< 2j\right\})$$

In MATLAB the implementation of this is easy. We can assume our image can be divided into multiple smaller grids. Then we simply compute the top left corner, top right corner, bottom left corner, and bottom right corner of each these grids. Then we can find the max of these values and we will have our down-sampled image. We will perform the maxpool operation on the following matrix A. The matrix is divided up into different color 2 by 2 grids.

\begin{center}
$A=$
\begin{tabular}{|c|c|c|c|}
	\hline
	\cellcolor{red!25}3 & \cellcolor{red!25}5 & \cellcolor{green!25}6 & \cellcolor{green!25}7\\ \hline
	\cellcolor{red!25}9 & \cellcolor{red!25}3 & \cellcolor{green!25}4 & \cellcolor{green!25}10\\ \hline
	\cellcolor{blue!25}5 & \cellcolor{blue!25}8 & 7 & 2\\ \hline
	\cellcolor{blue!25}1 & \cellcolor{blue!25}0 & 2 & 4\\ \hline
\end{tabular}
\end{center}

The maxpool operation on matrix A results in in the following matrix. We had to take the maximum value of each of these grids and then put them in their respective location. Now we have our down-sampled matrix which is half the size.

\begin{center}
\texttt{maxpool}$(A)=$
\begin{tabular}{|c|c|c|c|}
	\hline
	\cellcolor{red!25}9 & \cellcolor{green!25}10\\ \hline
	\cellcolor{blue!25}8 & 7\\ \hline
\end{tabular}
\end{center}

There are other pooling operations such as average pooling. This can also be computed in a similar way but instead of taking the maximum value of each grid, we take the average. This should still result in a matrix of half the size but will contain different values.

#### Convolution

The convolution layer convolves a set of filters, specified in “CNNparameter.mat”, over its input. A high filter response indicates similarity between the filter and the input image. Filter responses obtained in the convolution layers of a CNN enable it to make a decision about the class of the input image. It does linear transformation from input to output without changing the spatial size of the input image, but changing the number of channels in the output image. The convolution applies a predefined linear filters and bias values to input image to compute each channel. We defined the convolution operation as follow:

$$Out(:,:,l)=\sum\limits_{k=1}^{D_1}F_l(:,:,k) \times In(:,:,k)+b_l$$

We use convolution to modify the spatial frequency characteristics of an image. We create 32 × 32 empty matrix for each channel and sum the convolved value within each input channel and add the bias value by the end to computer each output channel for the image. The convolution layer was implemented in MATLAB based on the above equation using two nested for-loops. The MATLAB function “imfilter” performs convolution on the input image.

#### FullConnect

We use fullconnect after we finish all of the convolution and ReLU computations and reduction of image sizes. Fullconnect function is applied to distill all the results down to 10 numbers, one for each class. The fullconnect operation generally does dot products to multiply all of the values in each input image layer, sum up the results, and add the bias value for each filter Fl and bl. We accomplish this goal by the following way:

$$Out(1,1,l)=\sum\limits_{i=1}^N\sum\limits_{j=1}^M\sum\limits_{k=1}^{D_1}F_l(i,j,k)*In(i,j,k)+b_l$$

In another word, we convert the input image to 1D vector. As a result, the output is an array of size of 1×1×D. We can then apply the 10 numbers to softmax to convert to probability. The fullconnect layer was implemented in MATLAB based on the above equation using three nested for-loops to compute the lth channel of the output image as the scalar value.

#### Softmax

The end of a convolutional neural network is usually either a softmax classifier or a support vector machine. In our network, we were told to use a softmax classifier. The softmax classifier takes in an array of size 1 × 1 × D. We define the output the following way:

$$Out(1,1,k)=\frac{exp(In(1,1,k)-a)}{\sum\limits_{k=1}^Dexp(In(1,1,k)-a)}$$

where a is defined as:

$$a=max(In(1,1,k))$$

We use the a term to reduce the exponential work we need to compute each time. This will give us the same result and with just less computation needed. The output of this softmax classifier is an array of probabilities for each class. The highest probability in this array is the class you predict. The array should also sum up to 1 and all values should be between 0 and 1. In MATLAB we would compute the a value first using the max function and after that we can follow our mathematical definition.
