---
layout:     post
title:      "Classifying Images using a Convolutional Neural Network"
subtitle:   "Convolutional Neural Network"
date:       2016-09-26 05:00:00
author:     "Haojun"
header-img: "img/in-post/cnn/header.jpg"
catalog:    true
tags:
    - Computer Vision
    - Convolutional Neural Network
    - Matlab
---

<script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

## Introduction

#### Task Performed

In this project we created a convolutional neural network and used it to classify the CIFAR-10 dataset. We created a convolutional neural network using eighteen layers, consisting of six layer types. The layer types were image normalization, convolution, rectified linear units, maxpool, fully connected, and softmax. In the following report, we describe each of these different layers types in detail. We also describe and show observations and results for the CNN, classifying the CIFAR-10 images using provided filters and bias values. Finally, we test our CNN on external images, found online, and present our findings. We hope to achieve a better understanding of convolutional neural networks and their implementation after completing this report. For this specific project, we wish to see that our network will have a good prediction rate accuracy.

## Layer Types & Description

#### Image Normalization

The first step in our convolutional neural network is to normalize the input image we are given. Images are normally given as a set of RGB values. These values range from 0 to 255 for each RGB channel. We then want to normalize all of these values to a range from -0.5 to 0.5. To do this we simply can take the input image and apply the following transformation on it:

$$Out(i,j,k)=\frac{In(i,j,k)}{255.0}-0.5$$

For example, let’s say we were given an image pixel with the following values, (R, G, B) = (10, 128, 20). Let us apply this image normalization technique to see what we get. So we will have (10, 128, 20)/255.0 − 0.5, which gives us (−0.4608,0.0020,−0.4216) as our normalized values. As we can see all the values are are between -0.5 and 0.5. Also, notice that the G value was 128 which is nearly half of 255 and as a result we get a value very close to zero after normalization. We can now used this normalized value in the rest of our convolutional neural network. Normalization of data is a necessary and useful technique in most machine learning applications. In MATLAB, this step can be easily implemented and follows exactly from the transformation we have just described. We will just take in the input array and divide by 255 and subtract by 0.5.

#### Rectified Linear Unit
We use rectified linear units (ReLUs) to threshold the input. These are usually the next layer after convolutions in convolutional neural networks.  We define ReLUs the following way:

$$Out(i,j,k)=max(In(i,j,k),0)$$

So for any input we are given, we will make sure the output always has values 0 or greater. We will demonstrate this technique by showing you an example on a matrix. We have our original matrix, A, defined as follows:

$$
A=
\begin{bmatrix}
	\hline
	0.4 & 0.3 & -0.1\\
	0 & 0.1 & -0.2\\
	-0.3 & -0.4 & 0.2\\
\end{bmatrix}
$$

After we apply ReLU(A), we have the result below. The highlighted cells show where the actual thresholding has occurred. All of the negative values in our original matrix have now been set to 0 and everything else has been left alone.

$$
ReLU(A)=
\begin{bmatrix}
	\hline
	0.4 & 0.3 & 0\\
	0 & 0.1 & 0\\
	0 & 0 & 0.2\\
\end{bmatrix}
$$

In MATLAB, this can be implemented in one line using the max function of the input array and 0. You can see from this that MATLAB allows us to easily implement many of these functions in the way they are mathematically defined.

There are many other types of activation functions such as sigmoid, tanh, and maxout. ReLUs have been adopted recently, since they are easy to compute and they lead to faster convergence. In other activation functions, we are forced to compute exponential values which are a lot more computation- ally heavy than performing a max function. One problem with ReLUs has been the fact that neurons can stop activating with ReLUs. Once a neuron is not activating it is basically a useless neuron in the network. Leaky ReLUs have been used to combat this and are defined as follows:

$$Out(i,j,k)=max(In(i,j,k),In(i,j,k) \times 0.01)$$
